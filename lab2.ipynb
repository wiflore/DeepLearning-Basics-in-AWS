{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Create and train a convolutional neural network model using ResNet-20\n",
    "\n",
    "In this task, you will train a ResNet neural network with CIFAR-10 training data to classify an image into 10 known categories. The code is written in MXNet.\n",
    "\n",
    "[CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) and CIFAR-100 are labeled subsets of the 80 million tiny images dataset. They were collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.\n",
    "\n",
    "![](cifar-10.png)\n",
    "\n",
    "The CIFAR-10 dataset consists of 60,000 32x32 color images in 10 classes, with 6000 images per class. There are 50,000 training images and 10,000 test images:\n",
    "\n",
    "The dataset is divided into five training batches and one test batch, each with 10,000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. The following are general ways to work with image datasets\n",
    "\n",
    "- Classification\n",
    "- Localization\n",
    "- Segmentation\n",
    "- Scene classification\n",
    "- [Scene parsing](http://sceneparsing.csail.mit.edu/) to segment and parse an image into different image regions associated with semantic categories, such as sky, road, person, and bed\n",
    "\n",
    "If you want to learn more about deep learning on images, here is a good lecture: [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/slides/2016/winter1516_lecture8.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run each cell in this notebook by pressing **SHIFT + ENTER**. When the cell finishes running, the text to the left of the cell changes from **In [*]:** to **In [1]**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import argparse\n",
    "import logging\n",
    "import mxnet as mx\n",
    "import random\n",
    "from mxnet.io import DataBatch, DataIter\n",
    "import numpy as np\n",
    "import time\n",
    "import subprocess\n",
    "import errno\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gluoncv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from __future__ import division\n",
    "import argparse, time, logging, random, math\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "\n",
    "from mxnet import gluon, nd\n",
    "from mxnet import autograd as ag\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "\n",
    "from gluoncv.model_zoo import get_model\n",
    "from gluoncv.utils import makedirs, TrainingHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of cpus to use\n",
    "num_cpus = 1\n",
    "ctx = [mx.cpu(i) for i in range(num_cpus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    # Randomly flip the image horizontally\n",
    "    transforms.RandomFlipLeftRight(),\n",
    "    # Randomly jitter the brightness, contrast, and saturation of the image\n",
    "    transforms.RandomColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "    # Randomly add noise to the image\n",
    "    transforms.RandomLighting(0.1),\n",
    "    # Transpose the image from height*width*num_channels to num_channels*height*width\n",
    "    # and map values from [0, 255] to [0,1]\n",
    "    transforms.ToTensor(),\n",
    "    # Normalize the image with mean and standard deviation calculated across all images\n",
    "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = transforms.Compose([\n",
    "    # Transpose the image from height*width*num_channels to num_channels*height*width\n",
    "    # and map values from [0, 255] to [0,1]\n",
    "    transforms.ToTensor(),\n",
    "    # Normalize the image with mean and standard deviation calculated across all images\n",
    "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size for each cpu\n",
    "per_device_batch_size = 128\n",
    "# Number of data loader workers\n",
    "num_workers = 8\n",
    "# Calculate effective total batch size\n",
    "batch_size = per_device_batch_size * num_cpus\n",
    "\n",
    "# Set train=True for training data\n",
    "# Set shuffle=True to shuffle the training data\n",
    "train_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.CIFAR10(train=True).transform_first(transform_train),\n",
    "    batch_size=batch_size, shuffle=True, last_batch='discard', num_workers=num_workers)\n",
    "\n",
    "# Set train=False for validation data\n",
    "val_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.CIFAR10(train=False).transform_first(transform_test),\n",
    "    batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model CIFAR_ResNet20_v1, with 10 output classes, without pretrained weights\n",
    "net = get_model('cifar_resnet20_v1', classes=10, pretrained=False)\n",
    "net.initialize(mx.init.Xavier(), ctx = ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using stochastic gradient descent\n",
    "optimizer = 'sgd'\n",
    "\n",
    "# Set parameters\n",
    "optimizer_params = {'learning_rate': 0.01, 'wd': 0.0001, 'momentum': 0.9}\n",
    "\n",
    "# Define the trainer for net\n",
    "trainer = gluon.Trainer(net.collect_params(), optimizer, optimizer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmaxcrossentropy loss function\n",
    "loss_fn = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "# Use accuracy as the training metric\n",
    "train_metric = mx.metric.Accuracy()\n",
    "train_history = TrainingHistory(['training-acc', 'validation-acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, ctx, val_data):\n",
    "    '''\n",
    "    The test function to be used in the training data to check accuracy of the unseen data\n",
    "    Params:\n",
    "        ctx: Context describes the device type and ID on which computation should be carried out\n",
    "        val_data: Validation data to check the accuracy of unseen data\n",
    "    Returns:\n",
    "        metrics: Metric name and accuracy\n",
    "    '''\n",
    "    metric = mx.metric.Accuracy()\n",
    "    for i, batch in enumerate(val_data):\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "        outputs = [net(X) for X in data]\n",
    "        metric.update(label, outputs)\n",
    "    return metric.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "lr_decay_count = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    tic = time.time()\n",
    "    train_metric.reset()\n",
    "    train_loss = 0\n",
    "\n",
    "    # Loop through each batch of training data\n",
    "    for i, batch in enumerate(train_data):\n",
    "        # Extract data and label\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "\n",
    "        # AutoGrad\n",
    "        with ag.record():\n",
    "            output = [net(X) for X in data]\n",
    "            loss = [loss_fn(yhat, y) for yhat, y in zip(output, label)]\n",
    "\n",
    "        # Backpropagation\n",
    "        for l in loss:\n",
    "            l.backward()\n",
    "\n",
    "        # Optimize\n",
    "        trainer.step(batch_size)\n",
    "\n",
    "        # Update metrics\n",
    "        train_loss += sum([l.sum().asscalar() for l in loss])\n",
    "        train_metric.update(label, output)\n",
    "\n",
    "    name, acc = train_metric.get()\n",
    "    # Evaluate on validation data\n",
    "    name, val_acc = test(net, ctx, val_data)\n",
    "\n",
    "    # Update history and print metrics\n",
    "    train_history.update([acc, val_acc])\n",
    "    print('[Epoch %d] train=%f val=%f loss=%f time: %f' %\n",
    "        (epoch, acc, val_acc, train_loss, time.time()-tic))\n",
    "\n",
    "# Plot the metric scores\n",
    "train_history.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should have close to 80% validation accuracy after 10 epochs. But how does your model compare to other models out there?\n",
    "\n",
    "# Task 4: Compare different ResNet models\n",
    "\n",
    "In this task, you will compare four validation accuracies between two different models, ResNet-20 and ResNet-56, using the flags `pretrained=True` and `pretrained=False`. During a machine learning project, you can compare different models using a metric like accuracy, precision, or recall. In this case, use the accuracy metric on the validation data only.\n",
    "\n",
    "To start, use code from the previous task but wrap the code in the function `model_training_job()` so that you can call it using multiple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cpus = 1\n",
    "ctx = [mx.cpu(i) for i in range(num_cpus)]\n",
    "\n",
    "def model_training_job(model, epochs=10):\n",
    "    '''\n",
    "    The function describes the model training job with the specified model using the variable \"model\".\n",
    "    The function includes ingesting the data, creating the transforms, and defining the hyperparams\n",
    "    before you start your training loop.\n",
    "    Params:\n",
    "        model: initialized machine learning algorithm you are training\n",
    "        epochs: number of epochs to train the algorithm; default is 10\n",
    "    Returns:\n",
    "        training_history: history of metrics per epoch\n",
    "    '''\n",
    "    num_epochs = epochs\n",
    "    \n",
    "    transform_train = transforms.Compose([\n",
    "    # Randomly flip the image horizontally\n",
    "    transforms.RandomFlipLeftRight(),\n",
    "    # Randomly jitter the brightness, contrast, and saturation of the image\n",
    "    transforms.RandomColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "    # Randomly add noise to the image\n",
    "    transforms.RandomLighting(0.1),\n",
    "    # Transpose the image from height*width*num_channels to num_channels*height*width\n",
    "    # and map values from [0, 255] to [0,1]\n",
    "    transforms.ToTensor(),\n",
    "    # Normalize the image with mean and standard deviation calculated across all images\n",
    "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "    ])\n",
    "    \n",
    "    transform_test = transforms.Compose([\n",
    "    #transforms.Resize(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "    ])\n",
    "    \n",
    "    # Batch size for each cpu\n",
    "    per_device_batch_size = 128\n",
    "    # Number of data loader workers\n",
    "    num_workers = 8\n",
    "    # Calculate effective total batch size\n",
    "    batch_size = per_device_batch_size * num_cpus\n",
    "\n",
    "    # Set train=True for training data\n",
    "    # Set shuffle=True to shuffle the training data\n",
    "    train_data = gluon.data.DataLoader(\n",
    "        gluon.data.vision.CIFAR10(train=True).transform_first(transform_train),\n",
    "        batch_size=batch_size, shuffle=True, last_batch='discard', num_workers=num_workers)\n",
    "\n",
    "    # Set train=False for validation data\n",
    "    val_data = gluon.data.DataLoader(\n",
    "        gluon.data.vision.CIFAR10(train=False).transform_first(transform_test),\n",
    "        batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    \n",
    "    # Learning rate decay factor\n",
    "    lr_decay = 0.0001\n",
    "    # Epochs where learning rate decays\n",
    "    lr_decay_epoch = [80, 160, np.inf]\n",
    "    lr_decay_count = 0\n",
    "\n",
    "    # Using stochastic gradient descent\n",
    "    optimizer = 'sgd'\n",
    "    # Set parameters\n",
    "    optimizer_params = {'learning_rate': 0.01, 'wd': 0.0001, 'momentum': 0.9}\n",
    "\n",
    "    # Define the trainer for net\n",
    "    trainer = gluon.Trainer(model.collect_params(), optimizer, optimizer_params)\n",
    "    \n",
    "    # Define the loss function\n",
    "    loss_fn = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    \n",
    "    # Define the training metric \"accuracy\" using mx.metric.Accuracy()\n",
    "    train_metric = mx.metric.Accuracy()\n",
    "    train_history = TrainingHistory(['training-acc', 'validation-acc'])\n",
    "    \n",
    "    print(\"Starting Training\")\n",
    "    for epoch in range(epochs):\n",
    "        tic = time.time()\n",
    "        train_metric.reset()\n",
    "        train_loss = 0\n",
    "\n",
    "        # Loop through each batch of training data\n",
    "        for i, batch in enumerate(train_data):\n",
    "            #print(f'Epoch: {epoch} Batch: {i}')\n",
    "            # Extract data and label\n",
    "            data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "            label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "\n",
    "            # AutoGrad\n",
    "            with ag.record():\n",
    "                output = [model(X) for X in data]\n",
    "                loss = [loss_fn(yhat, y) for yhat, y in zip(output, label)]\n",
    "\n",
    "            # Backpropagation\n",
    "            for l in loss:\n",
    "                l.backward()\n",
    "\n",
    "            # Optimize\n",
    "            trainer.step(batch_size)\n",
    "\n",
    "            # Update metrics\n",
    "            train_loss += sum([l.sum().asscalar() for l in loss])\n",
    "            train_metric.update(label, output)\n",
    "\n",
    "        name, acc = train_metric.get()\n",
    "        # Evaluate on Validation data\n",
    "        name, val_acc = test(model,ctx, val_data)\n",
    "\n",
    "        # Update history and print metrics\n",
    "        train_history.update([acc, val_acc])\n",
    "        print('[Epoch %d] train=%f val=%f loss=%f time: %f' %\n",
    "            (epoch, acc, val_acc, train_loss, time.time()-tic))\n",
    "\n",
    "    # Plot the metric scores\n",
    "    train_history.plot()\n",
    "    return train_history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trains = {}\n",
    "\n",
    "print('Training cifar_resnet20_v2 without pretrain')\n",
    "net_20_f = get_model('cifar_resnet20_v2', classes=10, pretrained=False, ctx=ctx)\n",
    "net_20_f.initialize(mx.init.Xavier(), ctx = ctx)\n",
    "trains['cifar_resnet20_v2_f'] = model_training_job(net_20_f,3)\n",
    "\n",
    "print('Training cifar_resnet56_v2 without pretrain')\n",
    "net_56_f = get_model('cifar_resnet56_v2', classes=10, pretrained=False, ctx=ctx)\n",
    "net_56_f.initialize(mx.init.Xavier(), ctx = ctx)\n",
    "trains['cifar_resnet56_v2_f'] = model_training_job(net_56_f,3)\n",
    "\n",
    "print('Training cifar_resnet20_v2 with pretrain')\n",
    "net_20_t = get_model('cifar_resnet20_v2', classes=10, pretrained=True, ctx=ctx)\n",
    "#net_20_t.initialize(mx.init.Xavier(), ctx = ctx)\n",
    "trains['cifar_resnet20_v2_t'] = model_training_job(net_20_t,3)\n",
    "\n",
    "print('Training cifar_resnet56_v2 with pretrain')\n",
    "net_56_t = get_model('cifar_resnet56_v2', classes=10, pretrained=True, ctx=ctx)\n",
    "#net_56_t.initialize(mx.init.Xavier(), ctx = ctx)\n",
    "trains['cifar_resnet56_v2_t'] = model_training_job(net_56_t,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the algorithms, use the library `bokeh` to plot the different validation curves to see the accuracy between them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh\n",
    "from bokeh.plotting import figure, output_file, show,output_notebook\n",
    "output_notebook()\n",
    "def model_comparison(data_type):\n",
    "    p = figure(plot_width=800, \n",
    "               plot_height=400,\n",
    "               x_axis_label='Number of epochs',\n",
    "               y_axis_label=f'{data_type} Accuracy',\n",
    "               toolbar_location='above')\n",
    "    x = list(range(len(trains['cifar_resnet20_v2_f']['training-acc'])))\n",
    "    colors = ['green', 'orange', 'blue','red']\n",
    "    color = colors[:len(trains.keys())]\n",
    "\n",
    "    for keys,col in zip(trains.keys(),colors):\n",
    "        print(keys,col)\n",
    "        acc = trains[keys][f'{data_type}-acc']\n",
    "        p.line(x,acc, line_width=2,legend_label=keys,color=col)\n",
    "        p.circle(x,acc, line_width=2,color=col)\n",
    "        #show(p)\n",
    "\n",
    "    p.legend.location = 'bottom_right'\n",
    "    p.xaxis[0].ticker.desired_num_ticks = len(x)\n",
    "    show(p)    \n",
    "\n",
    "model_comparison('validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now look at the training data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comparison('training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "In the plot, `cifar_resnet20_v2_f` and `cifar_resnet56_v2_f` are very close to each other but aren't close to `cifar_resnet20_v2_t` and `cifar_resnet56_v2_t`. One difference to notice is that you added the flag `pretrained=True` to the models that are giving much higher accuracy than the other two models. \n",
    "\n",
    "### Question: Why do the models give a higher accuracy for `pretrained=True` flag? What is pretraining and the pretraining flag? \n",
    "\n",
    "**Answer**: A pretrained convolutional neural network (CNN) model is a CNN model that has been trained on a larger dataset for you and sometimes run for a longer time (more epochs). This lab's models were training on the CIFAR-10 dataset, and the initial weights that the model learned were added to your inital weights. So you start your training with features and the weights that the pretrained model learned instead of learning from scratch. This is also called *incremental training*. \n",
    "\n",
    "In most cases, the problem you are working on may not be exactly the same problem as one of these datasets. For example, what if the classes that you are trying to predict are not in the CIFAR-10 dataset? In such a case, you can still use a pretrained model from a related large-scale problem such as ImageNet for other visual recognition tasks without the need to train the first few layers. In this case, the first layer weights are fixed or unchanged while you train the model to recognize the images for your problem. This is called *fine tuning*.\n",
    "\n",
    "The upper layers are trained or fine tuned to match your problem at hand. This transfer of knowledge from one problem to another problem is called *transfer learning* because you are using a CNN model that was trained on a different but correlated problem. This is normally done to speed up the learning and reduces the need for very large training datasets.\n",
    "\n",
    "## Task 5: Use Amazon SageMaker built-in algorithms to train your model incrementally\n",
    "\n",
    "Now take the model you trained and use the Amazon SageMaker image classification algorithm to train the model. This algorithm is a supervised learning algorithm that supports multi-label classification. It takes an image as input and outputs one or more labels assigned to that image. The algorithm uses a CNN (ResNet) that can be trained from scratch or trained using transfer learning when a large number of training images are not available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, save the parameters of your created model **(CIFAR_ResNet20_v1)** in Task 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.save_parameters('cifar10_resnet20_v2_f.params')\n",
    "#net.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** You will see the **cifar10_resnet20_v2_f.params** file created in the notebook instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, you may want to save the model params as well as the model architecture. If your network is hybrid, you can even save the network architecture into files, and you won’t need the network definition in a Python file to load the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here, you use the saved model **(CIFAR_ResNet20_v1)** in Task 3 and re-train it with 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.hybridize()\n",
    "model_training_job(net, 5)\n",
    "net.export('cifar10_resnet20_v2_f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training job on SageMaker Instances\n",
    "\n",
    "The Amazon SageMaker image classification algorithm supports both **RecordIO** (`application/x-recordio`) and **image** (`image/png`, `image/jpeg`, and `application/x-image`) content types for training in file mode and supports the **RecordIO** (`application/x-recordio`) content type for training in pipe mode. However, you can also train in pipe mode using the image files (`image/png`, `image/jpeg`, and `application/x-image`) without creating RecordIO files by using the augmented manifest format. The algorithm supports `image/png`, `image/jpeg`, and `application/x-image` for inference. You will use the RecordIO format that is already provided on this notebook instance in this lab.\n",
    "\n",
    "**Note:** For this lab, an Amazon Simple Storage Service (Amazon S3) bucket has been pre configured with the training and validation data so the Amazon SageMaker training job can access it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, get the right container image for image training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the right container image for image training\n",
    "import boto3\n",
    "import sagemaker\n",
    "import re\n",
    "from sagemaker import get_execution_role\n",
    "import logging\n",
    "from sagemaker import image_uris\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "training_image = image_uris.retrieve('image-classification',boto3.Session().region_name)\n",
    "print(\"Training Image: \", training_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you are finished with all the setup that is needed, you are ready to train the object detector. To begin, create a sageMaker.estimator.Estimator object. This estimator launches the training job.\n",
    "\n",
    "You need to set two kinds of parameters for training. The first are the parameters for the training job. These include:\n",
    "- **Training instance count**: Number of instances on which to run the training. When the number of instances is greater than one,  the image classification algorithm runs in distributed settings.\n",
    "- **Training instance type**: Type of machine on which to run the training. Typically, cpu instances are used for this training.\n",
    "- **Output path**: Amazon S3 folder in which the training output is stored\n",
    "\n",
    "Run the training using the Amazon SageMaker CreateTrainingJob API.\n",
    "\n",
    "**Note:** In the code below, Replace `<LabDataBucket>` value with the bucket name value from the left side of the lab instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import re\n",
    "from sagemaker import get_execution_role\n",
    "import logging\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "\n",
    "\n",
    "s3_output_location = 's3://{}/{}/output'.format('<LabDataBucket>', 'image-classification-full-training/output/image-classification')\n",
    "cifar = sagemaker.estimator.Estimator(training_image,\n",
    "                                         role, \n",
    "                                         instance_count=1, \n",
    "                                         instance_type='ml.m4.xlarge',\n",
    "                                         volume_size = 30,\n",
    "                                         max_run = 360000,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also hyperparameters that are specific to the algorithm. These are:\n",
    "- **num_layers**: Number of layers (depth) for the network. This sample uses 18, but other values such as 50 and 152 can be used.\n",
    "- **image_shape**: Input image dimensions,'num_channels, height, width', for the network. It should be no larger than the actual image size. The number of channels should be same as the actual image.\n",
    "- **num_classes**: Number of output classes for the new dataset. ImageNet was trained with 1,000 output classes, but the number of output classes can be changed for fine-tuning. For the Caltech 256 dataset, 257 is used because it has 256 object categories + 1 clutter class.\n",
    "- **num_training_samples**: Total number of training samples. It is set to 15,240 for the Caltech 256 dataset with the current split.\n",
    "- **mini_batch_size**: Number of training samples used for each mini batch. In distributed training, the number of training samples used per batch is N * mini_batch_size where N is the number of hosts on which training is run.\n",
    "- **epochs**: Number of training epochs\n",
    "- **learning_rate**: Learning rate for training\n",
    "- **top_k**: Report the top-k accuracy during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar.set_hyperparameters(num_layers=20, \n",
    "                             image_shape = \"3,32,32\",\n",
    "                             num_classes=10,\n",
    "                             num_training_samples=50000,\n",
    "                             mini_batch_size=128,\n",
    "                             epochs=10,\n",
    "                             learning_rate=0.1,\n",
    "                             top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you will see how to create a definition for input data used by an Amazon SageMaker training job.\n",
    "\n",
    "**Note:** In the code below, for **s3_train** and **s3_validation** variables, replace `<LabDataBucket>` value with the bucket name value from the left side of the lab instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the path of training data that is uploaded to the S3 bucket.\n",
    "s3_train = 's3://<LabDataBucket>/image-classification-full-training/train/'\n",
    "\n",
    "# Get the path of validation data that is uploaded to the S3 bucket.\n",
    "s3_validation = 's3://<LabDataBucket>/image-classification-full-training/validation/'\n",
    "\n",
    "train_data = sagemaker.inputs.TrainingInput(s3_train, distribution='FullyReplicated', \n",
    "                            content_type='application/x-recordio', s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.inputs.TrainingInput(s3_validation, distribution='FullyReplicated', \n",
    "                            content_type='application/x-recordio', s3_data_type='S3Prefix')\n",
    "\n",
    "data_channels = {'train': train_data, 'validation': validation_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start model training, estimator's fit method with the training and validation data is called. \n",
    "\n",
    "To learn more about this, select this link: [Estimator fit method for training](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html#sagemaker.estimator.EstimatorBase.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** For this lab, the model artifact (model.tar.gz) from the training job has already been created and saved to an Amazon Simple Storage Service (Amazon S3) bucket.\n",
    "\n",
    "\n",
    "Run the code below to see the created model.\n",
    "Replace `<LabDataBucket>` value with the bucket name value from the left side of the lab instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls s3://<LabDataBucket>/image-classification-full-training/output/image-classification/output/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Prepare your model for inference using an Amazon SageMaker endpoint\n",
    "\n",
    "Now you can use the trained model to perform inference. For this example, that means predicting the 10 classes in the CIFAR-10 dataset. You can deploy the created model by using the deploy method in the estimator. This creates a new Amazon SageMaker endpoint. You can deploy it to get predictions in one of two ways:\n",
    "- To set up a persistent endpoint to get one prediction at a time, use Amazon SageMaker hosting services.\n",
    "- To get predictions for an entire dataset, use Amazon SageMaker batch transform.\n",
    "\n",
    "In this task, you will use Amazon SageMaker hosting services to set up a persistent endpoint to get a single prediction per call.\n",
    "\n",
    "Deploying a model using Amazon SageMaker hosting services is a three-step process:\n",
    "\n",
    "1. **Create a model in Amazon SageMaker**: By creating a model, you tell Amazon SageMaker where it can find the model components. This includes the Amazon S3 path where the model artifacts are stored and the Docker registry path for the image that contains the inference code. In subsequent deployment steps, you specify the model by name.\n",
    "\n",
    "2. **Create an endpoint configuration for an HTTPS endpoint**: You specify the name of one or more models in production variants and the ML compute instances that you want Amazon SageMaker to launch to host each production variant.\n",
    "\n",
    "3. **Create an HTTPS endpoint**: Provide the endpoint configuration to Amazon SageMaker. The service launches the ML compute instances and deploys the model or models as specified in the configuration. For more information, see the CreateEndpoint API. To get inferences from the model, client applications send requests to the Amazon SageMaker Runtime HTTPS endpoint. For more information about the API, see the InvokeEndpoint API.\n",
    "\n",
    "**Note:** In the code below, Replace `<LabDataBucket>` value with the bucket name value from the left side of the lab instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "\n",
    "# Get SageMaker execution role \n",
    "sagemaker_role = get_execution_role()\n",
    "\n",
    "# Get the model location path from the S3 bucket\n",
    "model_url='s3://<LabDataBucket>/image-classification-full-training/output/image-classification/output/model.tar.gz'\n",
    "\n",
    "model = Model(image_uri=training_image, \n",
    "            model_data=model_url, \n",
    "            role=sagemaker_role)\n",
    "\n",
    "cifar_classifier = model.deploy(initial_instance_count=1,\n",
    "                            instance_type='ml.m4.xlarge',\n",
    "                            endpoint_name='cifar-image-classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a currently deployed endpoint, you can update the endpoint with the following command. To do this, uncomment the command, and replace `<endpoint_name>` with the name of your currently running endpoint. Wait until the endpoint is updated before running the next code cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cifar_classifier = cifar.deploy(endpoint_name = <endpoint_name>, \n",
    "#                                update_endpoint=True, \n",
    "#                                initial_instance_count = 1, \n",
    "#                                instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check whether the endpoint has updated or created, use **boto3** to `DescribeEndpoint`. Do not continue until the status changes to **InService**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = boto3.client('sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe_endpoint = sm.describe_endpoint(EndpointName='cifar-image-classification')\n",
    "print(f\"The status of the endpoint is {describe_endpoint['EndpointStatus']} \")\n",
    "describe_endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you already have the validation data in the variable `val_data`. Import the raw image data `img_data` from Gluon, which you will use for prediction. Use this endpoint in two different ways to predict:\n",
    "1. Predict with raw Gluon CIFAR-10 validation image data `img_data` when you are developing your model.\n",
    "2. Predict with a URL when you deploy your model in your app and your app gets an image URL.\n",
    "\n",
    "### Predict with raw Gluon CIFAR-10 validation image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import autograd, gluon, image, init, nd\n",
    "from matplotlib.pylab import imshow\n",
    "\n",
    "img_data = gluon.data.vision.CIFAR10(train=False)\n",
    "\n",
    "label_dict = {0:\"airplane\", 1:\"automobile\", 2:\"bird\", 3:\"cat\", 4:\"deer\",\n",
    "              5:\"dog\", 6:\"frog\", 7:\"horse\", 8:\"ship\", 9:\"truck\"\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Amazon SageMaker endpoint predicts one image at a time. Choose the first image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = img_data[0]\n",
    "data = sample[0]\n",
    "label = sample[1]\n",
    "\n",
    "imshow(data.asnumpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json\n",
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "runtime= boto3.client('runtime.sagemaker')\n",
    "endpoint = 'cifar-image-classification'\n",
    "\n",
    "sample_imgs, sample_labels = img_data[:10]\n",
    "\n",
    "for img, label in zip(sample_imgs, sample_labels): \n",
    "    payload = cv2.imencode('.jpeg', img.asnumpy())[1].tobytes()    \n",
    "    response = runtime.invoke_endpoint(EndpointName=endpoint,\n",
    "                                       ContentType='application/x-image',\n",
    "                                       Body= payload)\n",
    "    pred = np.argmax(json.loads(response['Body'].read()))\n",
    "    label_dict[pred]\n",
    "\n",
    "    print(f\"Prediction: {pred}-{label_dict[pred]}, True Label: {label}-{label_dict[label]} \" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict with a URL\n",
    "\n",
    "For developing your application, the `model.predict` works perfectly. However, when you deploy your model to your application, you need to use the `sagemaker-runtime` library and call the `invoke_endpoint` API to get the predictions.\n",
    "\n",
    "**Note** `response['Body'].read()` can only be called once each time you call the `invoke_endpoint` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from IPython.display import Image\n",
    "\n",
    "urls = 'https://cdn.pixabay.com/photo/2013/06/08/04/17/ferry-boat-123059__340.jpg'\n",
    "\n",
    "display(Image(requests.get(urls).content))\n",
    "\n",
    "\n",
    "payload = requests.get(urls).content\n",
    "\n",
    "ENDPOINT_NAME = 'cifar-image-classification'\n",
    "runtime= boto3.client('runtime.sagemaker')\n",
    "response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n",
    "                                       ContentType='application/x-image',\n",
    "                                       Body=payload)\n",
    "\n",
    "pred = np.argmax(json.loads(response['Body'].read()))\n",
    "label_dict[pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab complete\n",
    "\n",
    "Congratulations! You have completed this lab. To clean up your lab environment, do the following:\n",
    "\n",
    "- Close this notebook file.\n",
    "- Log out of Jupyter Notebook by clicking **Quit**. Then, close the tab.\n",
    "- Log out of the AWS Management Console by clicking the user name at the top of the console, and then clicking **Sign Out**.\n",
    "- End the lab session by clicking **End Lab**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
